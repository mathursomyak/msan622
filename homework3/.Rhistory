geom_line(aes(y=FTSE,color="FTSE"))+
ggtitle("Financial Indecies Trends")+
theme(plot.title = element_text(face="bold", size = 14))+
scale_color_discrete("Financial\nIndex")+
ylab="price"
ggplot(data = eu, aes(x=time))+
geom_line(aes(y=DAX,color="DAX"))+
geom_line(aes(y=CAC,color="CAC"))+
geom_line(aes(y=SMI,color="SMI"))+
geom_line(aes(y=FTSE,color="FTSE"))+
ggtitle("Financial Indecies Trends")+
theme(plot.title = element_text(face="bold", size = 14))+
scale_color_discrete("Financial\nIndex")+
ylab=("price")
ggplot(data = eu, aes(x=time))+
geom_line(aes(y=DAX,color="DAX"))+
geom_line(aes(y=CAC,color="CAC"))+
geom_line(aes(y=SMI,color="SMI"))+
geom_line(aes(y=FTSE,color="FTSE"))+
ggtitle("Financial Indecies Trends")+
theme(plot.title = element_text(face="bold", size = 14))+
scale_color_discrete("Financial\nIndex")+
ylab("price")
hw1.multiline <-
ggplot(data = eu, aes(x=time))+
geom_line(aes(y=DAX,color="DAX"))+
geom_line(aes(y=CAC,color="CAC"))+
geom_line(aes(y=SMI,color="SMI"))+
geom_line(aes(y=FTSE,color="FTSE"))+
ggtitle("Financial Indecies Trends")+
theme(plot.title = element_text(face="bold", size = 14))+
scale_color_discrete("Financial\nIndex")+
ylab("price")
ggsave(filename = "hw1-multiline.png", plot = hw1.multiline)
ggsave(filename = "hw1-multiline.png", plot = hw1.multiline)
library (ggmap)
install.packages('ggmap')
library (ggmap)
usa<-map_data('usa')
usa
p <- ggplot(legend=FALSE) +
geom_polygon(data=world, aes(x=long, y=lat,group=group)) +
theme(panel.background = element_blank()) +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.text.x = element_blank(),axis.text.y = element_blank()) +
theme(axis.ticks = element_blank()) +
xlab("") + ylab("")
p <- ggplot(legend=FALSE) +
geom_polygon(data=usa, aes(x=long, y=lat,group=group)) +
theme(panel.background = element_blank()) +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.text.x = element_blank(),axis.text.y = element_blank()) +
theme(axis.ticks = element_blank()) +
xlab("") + ylab("")
p
p <- ggplot(legend=FALSE) +
geom_polygon(data=usa, aes(x=long, y=lat,group=group)) +
theme(panel.background = element_blank()) +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()) +
xlab("") + ylab("")
usa<-map_data('state')
#sf<-data.frame(long=-122.26,lat=37.47)
p <- ggplot(legend=FALSE) +
geom_polygon(data=usa, aes(x=long, y=lat,group=group)) +
theme(panel.background = element_blank()) +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()) +
xlab("") + ylab("")
p
happyData <- read.csv(file.choose(), header=T)
View(happyData)
mappy <- merge(happyData, usa, by= 'region')
usa
View(usa)
happyData <- read.csv(file.choose(), header=T)
View(happyData)
mappy <- merge(happyData, usa, by= 'region')
mappy <- arrange(mappy,order)
library (plyr)
mappy <- arrange(mappy,order)
View(mappy)
states <- data.frame(state.center, state.abb)
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = Sentiment(rate,5)))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = Sentiment))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = round(Sentiment*10))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon()#(aes(fill = round(Sentiment*10))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon()+#(aes(fill = round(Sentiment*10))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
typeof(mappy)
class(mappy)
mappy$senti <- ceil(mappy$Sentiment * 10)
mappy$senti <- ceiling(mappy$Sentiment * 10)
View(mappy)
mappy$senti <- ceiling(mappy$Sentiment * 100)
View(mappy)
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = as.factor(senti))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("Sentiment", palette = 'PuRd')+
coord_map()
s
mappy$senti <- as.factor(ceiling(mappy$Sentiment * 100))
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti)+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("senti", palette = 'PuRd')+
coord_map()
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti)+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("senti", palette = 'PuRd')
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("senti", palette = 'PuRd')
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti))+
geom_path(color = 'grey', linestyle = 2)
s
View(happyData)
happyData <- read.csv(file.choose(), header=T)
usa<-map_data('state')
mappy <- merge(happyData, usa, by= 'region')
mappy <- arrange(mappy,order)
mappy$senti <- as.factor(ceiling(mappy$Sentiment * 100))
states <- data.frame(state.center, state.abb)
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("senti", palette = 'PuRd')
s
s <- ggplot(data = mappy, aes(x=long, y=lat, group = group)) +
geom_polygon(aes(fill = senti))+
geom_path(color = 'grey', linestyle = 2)+
scale_fill_brewer("sentiment", palette = 'PuRd')
s
data <- read.csv(file.choose(), header = T, sep=",")
View(data)
data <- read.csv(file.choose(), header = F,sep=",")
View(data)
library(psych)
f <-factanal(x=formula, data=data, covmat=as.matrix(data), n.obs=220, factors=2, rotation="none")
summary(f)
f$loadings
g <-factanal(x=formula, data=data, covmat=as.matrix(data), n.obs=220, factors=2, rotation="verimax")
g <-factanal(x=formula, data=data, covmat=as.matrix(data), n.obs=220, factors=2, rotation="varimax")
g$loadings
data <- read.csv("C:\Users\skmathur\Documents\Analytics\MultiVariateStats\HW1\MicrosoftFactorData.csv")
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/MicrosoftFactorData.csv")
View(data)
View(data[,6:11])
data <- (data[,6:11])
names(data)
myPCA <- princomp(~USTB3M+USTB6M+USTB1Y+USTB3Y+USTB5Y+USTB10Y,data=data)
summary(myPCA)
scree(myPCA)
screeplot(myPCA)
screeplot(myPCA)
eigen(data)
eigen(corr(data))
eigen(corr(as.matrix(data)))
eigen(cor(as.matrix(data)))
names(data)
myPCA$loadings
library(psych)
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/CorrelationMatrixBodyMeasurements.csv", header=FALSE, sep=",")
myPCA <- principal(r=data, nfactors=8)
View(data)
# PART A:
#----------
summary(myPCA)
plot(myPCA$values)
"Based on the 'elbow' on the scree plot, we need only the first 2 principal components."
# PART B:
#----------
myPCA$loadings
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/MicrosoftFactorData.csv")
data <- (data[,6:11])
names(data)
myPCA <- princomp(~USTB3M+USTB6M+USTB1Y+USTB3Y+USTB5Y+USTB10Y,data=data)
summary(myPCA)
Sigma <- matrix(c(5,2,2,2),nrow=2,ncol=2)
View(Sigma)
eigen(Sigma)
eigen(Sigma)$values
Corr <- cov2cor(Sigma)
View(Corr)
eigen(Corr)
eigen(Corr)$values
eigen(Corr)$values/sum(eigen(Corr)$values)
6/7
eigvec <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
eigvec
data <- read.csv("C:\Users\skmathur\Documents\Analytics\MultiVariateStats\HW1\BartlettTest.csv")
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/BartlettTest.csv")
View(data)
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/BartlettTest.csv",header=T)
View(data)
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/BartlettTest.csv",header=F)
View(data)
corr(data)
cor(data)
C <- cor(data)
bartlette.test(data)
bartlett.test(data)
View(data)
data_o <- rbind(data,c(10800,12000,10000,10000))
bartlett.test(data_o)
S = matrix(c(10005.2, 255.76, 255.76, 14.30), nrow=2, ncol=2)
S
eigen(S)
eigen(S)$vectors
eigen(S)$values
"The principal components are:"
eigen(S)$vectors
"The variance for each PC is:"
eigen(S)$values
eigen(S)$values/sum(eigen(S)$values)
library(ggplot2)
ggplot()+geom_abline(a=-1/0.03, b=-.99/0.03)
ggplot(data)+geom_abline(a=-1/0.03, b=-.99/0.03)
ggplot(data)+geom_abline(aes(a=-1/0.03, b=-.99/0.03))
eigen(S)$values
eigen(S)$vectors
x_sales <- rnorm(n=1000, mean= 62309, sqrt(10005.2))
x_profit <- rnorm(n=1000, mean= 2927, sqrt(14.3))
data <- cbind(x_sales,x_profit)
View(data)
ggplot(data)+geom_abline(aes(a=-1/0.03, b=-.99/0.03))
typeof(data)
class(data)
data <- as.data.frame(cbind(x_sales,x_profit))
ggplot(data)+geom_abline(aes(a=-1/0.03, b=-.99/0.03))
geom_abline(data,aes(a=-1/0.03, b=-.99/0.03))
geom_abline(data=data,aes(a=-1/0.03, b=-.99/0.03))
geom_abline(data=data,aes(a=-1/0.03, b=-.99/0.03))
plot((-1/0.03)-(.99/0.03)*x_sales)
plot(x_sales,(-1/0.03)-(.99/0.03)*x_sales)
ggplot(data=data, aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales))))+geom_line()
ggplot(data=data, aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)))+geom_line()
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)))+
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit)))
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)))+
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit)))
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)))
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)))+
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit)))+
xlim(0,50000)+ ylim(0,5000)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 100)+
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit)))+
xlim(-50000,500000)+ ylim(-50000,5000)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 100)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 10)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 10) +
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit)))+
xlim(-50000,500000)+ ylim(-5000000,5000)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 10) +
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit), size = 10))+
xlim(-50000,500000)+ ylim(-5000000,5000)
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 10) +
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit), size = 10))
ggplot(data=data)+
geom_line(aes(x=x_sales, y=((-1/0.03)-(.99/0.03)*x_sales)), size = 1) +
geom_line(aes(x=x_profit, y=((-1/0.93)+(.03/0.99)*x_profit), size = 1))
install.packages("ellipse")
library(ellipse)
ella <- ellipse(S,centre=x_bar,level = 0.5)
plot(ella)
plot(ella, type='l',)
library(ellipse)
ella <- ellipse(S,centre=x_bar,level = 0.5)
x_bar = c(62309, 2927)
ella <- ellipse(S,centre=x_bar,level = 0.5)
ella <- ellipse(S,centre=x_bar,level = 0.5)
plot(ella, type='l',)
par(new=T)
plot(S$vectors[,1], type='l', xaxt='n', ann=F, yaxt='n')
par(new=T)
plot(S$vectors[,2], type='l', xaxt='n', ann=F, yaxt='n')
plot(S$vectors[,1], type='l', xaxt='n', ann=F, yaxt='n')
ella <- ellipse(S,centre=x_bar,level = 0.5)
plot(ella, type='l',)
par(new=T)
plot(eigen(S)$vectors[,1], type='l', xaxt='n', ann=F, yaxt='n')
par(new=T)
plot(eigen(S)$vectors[,2], type='l', xaxt='n', ann=F, yaxt='n')
library(stats)
#==============
# QUESTION 1
#==============
data <- read.csv(file.choose(), header=FALSE, sep=",")
data <- data[2:10] #remove NA column
#names(data) <- c("100M", "200M", "400M", "800M", "1500M", "5KM", "10KM", "Marathon", "Country")
names(data) <- c("M100", "M200", "M400", "M800", "M1500", "KM5", "KM10", "Marathon", "Country")
# PART A:
#----------
data[4:8] <- (data[4:8])*60 #converting all fields to seconds
myPCA <- princomp(~M100+M200+M400+M800+M1500+KM5+KM10+Marathon,data=data)
summary(myPCA)
"Only the first principal component is needed to explain 99.5% of the total variation"
# PART B:
#----------
myPCA$loadings
# PART C:
#----------
screeplot(myPCA)
# PART D:
#----------
summary(myPCA)
colSums(myPCA$loadings)
# PART E:
#----------
#myPCA$sdev^2/(sum(myPCA$dev^2))
"PC1 = 0.98(Marathon) + 0.18(10K_time)"
# PART F:
#----------
#check if sum of squares for comp1 = 1
sum(myPCA$loadings[,1]**2)
# PART G:
#----------
#View(myPCA$scores)
max_idx<- apply(myPCA$scores[,1:ncol(myPCA$scores)], 2, function(x) which.max(x))
min_idx<- apply(myPCA$scores[,1:ncol(myPCA$scores)], 2, function(x) which.min(x))
maxes <- data["Country"][max_idx,]
mins  <- data["Country"][min_idx,]
min_max <- data.frame(maxes,mins)
rownames(min_max) <- names(as.data.frame(myPCA$scores))
min_max
#==============
# QUESTION 2
#==============
library(psych)
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/CorrelationMatrixBodyMeasurements.csv", header=FALSE, sep=",")
myPCA <- principal(r=data, nfactors=8)
View(data)
# PART A:
#----------
summary(myPCA)
plot(myPCA$values)
"Based on the 'elbow' on the scree plot, we need only the first 2 principal components."
# PART B:
#----------
myPCA$loadings
"The first component is a representation of the first 4 original factors"
"The second component is mostly original factor 7"
"The third component is mostly original factor 6"
"The fourth component is mostly orginal factor 8"
# PART C:
#----------
communality <- myPCA$communality #rowSums(myPCA$loadings**2)
uniqueness <- 1- communality
"V1 has the lowest uniqueness, and v7 has the highest. All are pretty close though."
# PART D:
#----------
ddata <- as.matrix(data)
loads <- as.matrix(myPCA$loadings)
colSums(ddata%*%loads)
#==============
# QUESTION 3
#==============
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/NormedMeasurementsPrehistoricGobletsThailand.csv")
View(data[3:9])
myPCA <- princomp(data[3:9])
summary(myPCA)
myPCA$loadings
biplot(myPCA)
#==============
# QUESTION 4
#==============
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/MicrosoftFactorData.csv")
data <- (data[,6:11])
myPCA <- princomp(~USTB3M+USTB6M+USTB1Y+USTB3Y+USTB5Y+USTB10Y,data=data)
summary(myPCA)
screeplot(myPCA)
eigen(cor(as.matrix(data)))
myPCA$loadings
"Both the scree plot and eigen values suggest using only the first principal component. The first PC is
the only one with eigenvalue greater than 1. 94% of the variance is explained with only the first PC. It
seems to represent an average of the original variables."
#==============
# QUESTION 5
#==============
Sigma <- matrix(c(5,2,2,2),nrow=2,ncol=2)
eigen(Sigma)$values
"The proportion of total variance explained by the 1st PC is 6/7, and 2nd PC is 1/7"
#==============
# QUESTION 6
#==============
Corr <- cov2cor(Sigma)
# PART A:
#----------
eigen(Corr)$values/sum(eigen(Corr)$values)
"Y1 = 0.707*X1 - 0.707*X2
Y2 = 0.707*X1 + 0.707*X2"
# PART B:
#----------
"6A answers are different from question 5 answers because the correlation matrix is a normalized version
of the covariance matrix. The eigen values in 6a are less extreme."
#==============
# QUESTION 7
#==============
"Y3 = X3
Y2 = X2
Y1 = X1"
"The eigen vectors are as follows:"
eigvec <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
eigvec
#==============
# QUESTION 8
#==============
S = matrix(c(10005.2, 255.76, 255.76, 14.30), nrow=2, ncol=2)
x_bar = c(62309, 2927)
# PART A:
#----------
"The principal components are:"
eigen(S)$vectors
"The variance for each PC is:"
eigen(S)$values
# PART B:
#----------
"Portion of Variance explained by each PC:"
eigen(S)$values/sum(eigen(S)$values)
# PART C:
#----------
install.packages("ellipse")
library(ellipse)
ella <- ellipse(S,centre=x_bar,level = 0.5)
plot(ella, type='l',)
par(new=T)
plot(eigen(S)$vectors[,1], type='l', xaxt='n', ann=F, yaxt='n')
par(new=T)
plot(eigen(S)$vectors[,2], type='l', xaxt='n', ann=F, yaxt='n')
# PART D:
#----------
"The first PC looks like it's mostly taken from sales. And the 2nd PC is a contrast
of the sales and profit"
#==============
# QUESTION 9
#==============
data <- read.csv("C:/Users/skmathur/Documents/Analytics/MultiVariateStats/HW1/BartlettTest.csv",header=F)
# PART A:
#----------
C <- cor(data)
bartlett.test(data)
"Here, we fail to reject the null hypothesis that rho = constant. By looking at the correlation matrix
we see that this makes sense intuitively: many off diagonal rho values are close together."
# PART B:
#----------
data_o <- rbind(data,c(10800,12000,10000,10000))
bartlett.test(data_o)
"The p-value of the data with the added outlier is very low. Which means we'd now reject the null hypothesis
So this test is sensitive to outliers."
install.packages("ellipse")
library(shiny)
library(ggplot2)
library(RColorBrewer)
library(scales)
library(grid)
require(GGally)
require(plyr)
runApp()
setwd("~/GitHub/msan622/homework3")
runApp()
